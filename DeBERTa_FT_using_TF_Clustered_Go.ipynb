{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pip"
      ],
      "metadata": {
        "id": "db_vjvou2YpT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kP5kFKdcsqmR"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers==4.29.0\n",
        "# !pip install datasets\n",
        "# !pip install huggingface-hub\n",
        "# !pip install tensorflow==2.12.0 keras==2.12.0\n",
        "# ! pip uninstall tensorflow\n",
        "# ! pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModel, TFDebertaV2ForSequenceClassification\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, f1_score, precision_recall_curve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52U6wQMJvAyU",
        "outputId": "466ffd3c-6fba-4caa-ed9a-0af90157a64e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-07 23:13:09.183854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-07 23:13:09.302768: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-07 23:13:09.306480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:09.306492: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-11-07 23:13:10.114237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:10.114293: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:10.114299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PZDk-FM0LLJ",
        "outputId": "78f0973b-fd7a-4340-8baf-aa1016cc1a5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  7 23:13:13 2024       \r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
            "|-----------------------------------------+------------------------+----------------------+\r\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                                         |                        |               MIG M. |\r\n",
            "|=========================================+========================+======================|\n",
            "|   0  Quadro RTX 6000                Off |   00000000:1B:00.0 Off |                  Off |\n",
            "| 33%   34C    P8             14W /  260W |       3MiB /  24576MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  Quadro RTX 6000                Off |   00000000:1C:00.0 Off |                  Off |\n",
            "| 33%   32C    P8              8W /  260W |   11295MiB /  24576MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   2  Quadro RTX 6000                Off |   00000000:1D:00.0 Off |                  Off |\n",
            "| 33%   38C    P8             35W /  260W |       3MiB /  24576MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   3  Quadro RTX 6000                Off |   00000000:1E:00.0 Off |                  Off |\n",
            "| 33%   38C    P8             24W /  260W |       3MiB /  24576MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   4  Quadro RTX 6000                Off |   00000000:3D:00.0 Off |                  Off |\n",
            "| 67%   84C    P0            176W /  260W |   19457MiB /  24576MiB |     96%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   5  Quadro RTX 6000                Off |   00000000:3F:00.0 Off |                  Off |\n",
            "| 51%   72C    P0            186W /  260W |   19707MiB /  24576MiB |     79%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   6  Quadro RTX 6000                Off |   00000000:40:00.0 Off |                  Off |\n",
            "| 45%   68C    P0            169W /  260W |   19707MiB /  24576MiB |    100%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   7  Quadro RTX 6000                Off |   00000000:41:00.0 Off |                  Off |\n",
            "| 68%   84C    P0            167W /  260W |   19687MiB /  24576MiB |     95%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    1   N/A  N/A   3662951      C   python                                      11288MiB |\n",
            "|    4   N/A  N/A    266948      C   ...mm5390/anaconda3/envs/LP/bin/python      19454MiB |\n",
            "|    5   N/A  N/A    266949      C   ...mm5390/anaconda3/envs/LP/bin/python      19704MiB |\n",
            "|    6   N/A  N/A    266950      C   ...mm5390/anaconda3/envs/LP/bin/python      19704MiB |\n",
            "|    7   N/A  N/A    266951      C   ...mm5390/anaconda3/envs/LP/bin/python      19684MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ######Absolutely hide before submitting########\n",
        "\n",
        "# login(token='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')"
      ],
      "metadata": {
        "id": "Lhq8sxCjvM3b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "read the data from drive and remove empty values"
      ],
      "metadata": {
        "id": "SRDo734wxof5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/uc?export=download&id=103IC2SS2II9Q0jdin4rTAXNiw3VshQ9Z\"\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "data = data.dropna(subset=['text_stopwords', 'written_text', 'emotions'])\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BWyN0NMHv6A_",
        "outputId": "b95c8e12-383a-418a-a515-f649c12f17d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      text_stopwords               emotions  \\\n",
              "0              wife board insists eating real dinner          ['amusement']   \n",
              "1  awful video wanted jump real cut start followe...  ['desire', 'disgust']   \n",
              "2  imagining drug stay ping ponging air drone kee...         ['excitement']   \n",
              "3  completely correct might lost control blown ti...               ['fear']   \n",
              "4                   file keep phone accessing within             ['caring']   \n",
              "\n",
              "                                        written_text  cluster  \n",
              "0  I couldn't get my wife on board. She insists o...       11  \n",
              "1  What an awful video. I just wanted to see the ...       14  \n",
              "2  I love imagining that the drugs just stay ping...        1  \n",
              "3  Completely correct. But he Might have lost con...       22  \n",
              "4  This probably not a good file to keep on your ...       11  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_stopwords</th>\n",
              "      <th>emotions</th>\n",
              "      <th>written_text</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wife board insists eating real dinner</td>\n",
              "      <td>['amusement']</td>\n",
              "      <td>I couldn't get my wife on board. She insists o...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>awful video wanted jump real cut start followe...</td>\n",
              "      <td>['desire', 'disgust']</td>\n",
              "      <td>What an awful video. I just wanted to see the ...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>imagining drug stay ping ponging air drone kee...</td>\n",
              "      <td>['excitement']</td>\n",
              "      <td>I love imagining that the drugs just stay ping...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>completely correct might lost control blown ti...</td>\n",
              "      <td>['fear']</td>\n",
              "      <td>Completely correct. But he Might have lost con...</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>file keep phone accessing within</td>\n",
              "      <td>['caring']</td>\n",
              "      <td>This probably not a good file to keep on your ...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if all columns are correct"
      ],
      "metadata": {
        "id": "W_M69lU3xd4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert {'text_stopwords', 'written_text', 'emotions', 'cluster'}.issubset(data.columns), \"Dataset must contain 'text_stopwords', 'written_text', 'emotions', 'cluster' columns.\""
      ],
      "metadata": {
        "id": "c16dc6RVw0el"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert emotions from string representation of lists to actual lists"
      ],
      "metadata": {
        "id": "vDjv0vMSxcfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(data['emotions'].iloc[0], str):\n",
        "    import ast\n",
        "    data['emotions'] = data['emotions'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "OBxwjk7ww9Yd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare emotion labels using MultiLabelBinarizer"
      ],
      "metadata": {
        "id": "v_nbIb2bxW1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize emotion labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "emotion_labels = mlb.fit_transform(data['emotions'])\n",
        "emotion_classes = mlb.classes_\n",
        "num_classes = len(emotion_classes)"
      ],
      "metadata": {
        "id": "GLuWyO60xVbx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conver clusters to int"
      ],
      "metadata": {
        "id": "hC3-X3XGxwLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize texts"
      ],
      "metadata": {
        "id": "moH8CArS5Oxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"kamalkraj/deberta-v2-xlarge\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qBX_YVWx2Pa",
        "outputId": "53a6e726-9ec3-456e-8104-13d6b7e2128f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/ssddata/home/mmv5513/colabenv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize 'written_text'\n",
        "encodings_written = tokenizer(\n",
        "    data['written_text'].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "# Tokenize 'text_stopwords'\n",
        "encodings_stopwords = tokenizer(\n",
        "    data['text_stopwords'].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,\n",
        ")"
      ],
      "metadata": {
        "id": "xkrTkfxRx6hx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare cluster input"
      ],
      "metadata": {
        "id": "QrS4nFbXx9dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare TensorFlow datasets\n",
        "features = {\n",
        "    'input_ids_written': encodings_written['input_ids'],\n",
        "    'attention_mask_written': encodings_written['attention_mask'],\n",
        "    'input_ids_stopwords': encodings_stopwords['input_ids'],\n",
        "    'attention_mask_stopwords': encodings_stopwords['attention_mask'],\n",
        "    'cluster': data['cluster'].astype(int).values,\n",
        "}\n",
        "labels = emotion_labels\n"
      ],
      "metadata": {
        "id": "9v5wGLSXyFom"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First input text:\", data['written_text'].iloc[0])\n",
        "print(\"First label:\", labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqg_Bj6RwICq",
        "outputId": "df1a968b-d8fb-49d1-8e57-d8be219706f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First input text: I couldn't get my wife on board. She insists on eating a \"real dinner\".\n",
            "First label: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample labels:\", labels[:5])\n",
        "print(\"Labels shape:\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sibbPjMMt0wY",
        "outputId": "e0673d3b-0af1-4ca0-dbb8-4022749c5ec5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample labels: [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Labels shape: (17966, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "label_sums = labels.sum(axis=0)\n",
        "print(\"Label counts per class:\", label_sums)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rClP4kZtuc9y",
        "outputId": "ca8d4706-1d5a-4bc1-a21a-3f84ecadde9c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label counts per class: [1414  947  943 1234 1287  912  871 1059  844 1164 1018  946  820  921\n",
            "  931 1102  670 1101  912  809  653 1118  713 1062  689  769 1139  861]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(len(labels))  # len(labels) should be 17966\n",
        "\n",
        "# Split indices\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize dictionaries to hold train and validation features\n",
        "train_features = {}\n",
        "val_features = {}\n",
        "\n",
        "# Split each feature array using the indices\n",
        "for key in features:\n",
        "    feature_array = np.array(features[key])\n",
        "    train_features[key] = feature_array[train_indices]\n",
        "    val_features[key] = feature_array[val_indices]\n",
        "\n",
        "# Split labels\n",
        "train_labels = labels[train_indices]\n",
        "val_labels = labels[val_indices]"
      ],
      "metadata": {
        "id": "8qxiG1x0vde3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Tensor dataset"
      ],
      "metadata": {
        "id": "cEjclkYdyN7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(features, labels, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "hhyosHAnyRP_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into training and validation sets"
      ],
      "metadata": {
        "id": "9ebzsxnXyLF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "train_dataset = create_dataset(train_features, train_labels, batch_size)\n",
        "val_dataset = create_dataset(val_features, val_labels, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYWspxUoyH2j",
        "outputId": "5c4bd4c1-98f8-40e2-b32a-9752720d6bfe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-07 23:13:20.467440: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:20.467562: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:20.467598: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:20.467628: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:20.468518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2024-11-07 23:13:20.468569: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2024-11-07 23:13:20.469263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Model"
      ],
      "metadata": {
        "id": "1_e4HAH8yVYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(physical_devices))\n",
        "print(\"GPUs:\", physical_devices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKfofdLzLseL",
        "outputId": "ab7f3287-85d0-48e8-e39d-6b3ca5af3947"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "GPUs: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "    # Model Definition\n",
        "    from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Embedding\n",
        "    from tensorflow.keras.models import Model\n",
        "    from transformers import TFDebertaV2ForSequenceClassification  # Updated import\n",
        "\n",
        "    # Cluster Embedding\n",
        "    num_clusters = data['cluster'].nunique()\n",
        "    cluster_embedding_dim = 16\n",
        "\n",
        "    # Inputs\n",
        "    input_ids_written = Input(shape=(None,), dtype=tf.int32, name='input_ids_written')\n",
        "    attention_mask_written = Input(shape=(None,), dtype=tf.int32, name='attention_mask_written')\n",
        "\n",
        "    input_ids_stopwords = Input(shape=(None,), dtype=tf.int32, name='input_ids_stopwords')\n",
        "    attention_mask_stopwords = Input(shape=(None,), dtype=tf.int32, name='attention_mask_stopwords')\n",
        "\n",
        "    cluster_input = Input(shape=(), dtype=tf.int32, name='cluster_input')\n",
        "\n",
        "    # Number of labels\n",
        "    num_labels = num_classes  # Assuming num_classes is defined elsewhere (28 in your case)\n",
        "\n",
        "    # DeBERTa V2 Models with Multi-Label Classification Heads\n",
        "    deberta_model_written = TFDebertaV2ForSequenceClassification.from_pretrained(\n",
        "        \"kamalkraj/deberta-v2-xlarge\",\n",
        "        num_labels=num_labels,\n",
        "        problem_type=\"multi_label_classification\",\n",
        "    )\n",
        "    deberta_model_stopwords = TFDebertaV2ForSequenceClassification.from_pretrained(\n",
        "        \"kamalkraj/deberta-v2-xlarge\",\n",
        "        num_labels=num_labels,\n",
        "        problem_type=\"multi_label_classification\",\n",
        "    )\n",
        "\n",
        "    # Get logits from both models\n",
        "    outputs_written = deberta_model_written(\n",
        "        input_ids=input_ids_written,\n",
        "        attention_mask=attention_mask_written,\n",
        "        training=True\n",
        "    )\n",
        "    logits_written = outputs_written.logits  # Shape: (batch_size, num_labels)\n",
        "\n",
        "    outputs_stopwords = deberta_model_stopwords(\n",
        "        input_ids=input_ids_stopwords,\n",
        "        attention_mask=attention_mask_stopwords,\n",
        "        training=True\n",
        "    )\n",
        "    logits_stopwords = outputs_stopwords.logits  # Shape: (batch_size, num_labels)\n",
        "\n",
        "    # Cluster Embedding\n",
        "    cluster_embedding = Embedding(\n",
        "        input_dim=num_clusters + 1,  # +1 if clusters start from 0\n",
        "        output_dim=cluster_embedding_dim,\n",
        "        name='cluster_embedding'\n",
        "    )\n",
        "    cluster_embeds = cluster_embedding(cluster_input)\n",
        "\n",
        "    # Concatenate all features\n",
        "    combined_output = Concatenate()([\n",
        "        logits_written,\n",
        "        logits_stopwords,\n",
        "        cluster_embeds\n",
        "    ])\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Dropout(0.3)(combined_output)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_labels, activation='sigmoid')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(\n",
        "        inputs=[\n",
        "            input_ids_written,\n",
        "            attention_mask_written,\n",
        "            input_ids_stopwords,\n",
        "            attention_mask_stopwords,\n",
        "            cluster_input\n",
        "        ],\n",
        "        outputs=outputs\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au4TZa9TyTOu",
        "outputId": "71889167-e608-456a-fa05-2e9663c8692f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:localhost/replica:0/task:0/device:GPU:1,/job:localhost/replica:0/task:0/device:GPU:0\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/ssddata/home/mmv5513/colabenv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFDebertaV2ForSequenceClassification.\n",
            "\n",
            "Some layers of TFDebertaV2ForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-v2-xlarge and are newly initialized: ['pooler', 'classifier', 'cls_dropout']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All model checkpoint layers were used when initializing TFDebertaV2ForSequenceClassification.\n",
            "\n",
            "Some layers of TFDebertaV2ForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-v2-xlarge and are newly initialized: ['pooler', 'classifier', 'cls_dropout']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /ssddata/home/mmv5513/colabenv/lib64/python3.9/site-packages/transformers/models/deberta_v2/modeling_tf_deberta_v2.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /ssddata/home/mmv5513/colabenv/lib64/python3.9/site-packages/tensorflow/python/ops/distributions/bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callbacks"
      ],
      "metadata": {
        "id": "GRYtFj5U6-PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "\n",
        "# Custom callback to compute F1 score at the end of each epoch\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super(F1ScoreCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.best_f1 = 0\n",
        "        self.best_weights = None\n",
        "        self.best_thresholds = None\n",
        "        self.patience = 3\n",
        "        self.epochs_no_improve = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_features, val_labels = self.validation_data\n",
        "        val_preds = self.model.predict(val_features)\n",
        "\n",
        "        optimal_thresholds = []\n",
        "        all_preds_bin = np.zeros_like(val_preds)\n",
        "\n",
        "        for i in range(num_classes):\n",
        "            precision, recall, thresholds = precision_recall_curve(val_labels[:, i], val_preds[:, i])\n",
        "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "            optimal_idx = np.argmax(f1_scores)\n",
        "            optimal_threshold = thresholds[optimal_idx]\n",
        "            optimal_thresholds.append(optimal_threshold)\n",
        "            all_preds_bin[:, i] = (val_preds[:, i] >= optimal_threshold).astype(int)\n",
        "\n",
        "        validation_f1 = f1_score(val_labels, all_preds_bin, average='micro')\n",
        "        print(f\"Epoch {epoch+1} - Validation F1 Score: {validation_f1:.4f}\")\n",
        "\n",
        "        print(classification_report(val_labels, all_preds_bin, target_names=emotion_classes))\n",
        "\n",
        "        if validation_f1 > self.best_f1:\n",
        "            self.best_f1 = validation_f1\n",
        "            self.best_weights = self.model.get_weights()\n",
        "            self.best_thresholds = optimal_thresholds.copy()\n",
        "            self.epochs_no_improve = 0\n",
        "            print(f\"Validation F1 increased to {validation_f1:.4f}, saving model weights...\")\n",
        "        else:\n",
        "            self.epochs_no_improve += 1\n",
        "            print(f\"No improvement in validation F1 for {self.epochs_no_improve} epoch(s)\")\n",
        "            if self.epochs_no_improve >= self.patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "f1_callback = F1ScoreCallback(\n",
        "    validation_data=(\n",
        "        {\n",
        "            'input_ids_written': np.array(val_features['input_ids_written']),\n",
        "            'attention_mask_written': np.array(val_features['attention_mask_written']),\n",
        "            'input_ids_stopwords': np.array(val_features['input_ids_stopwords']),\n",
        "            'attention_mask_stopwords': np.array(val_features['attention_mask_stopwords']),\n",
        "            'cluster_input': np.array(val_features['cluster']),\n",
        "        },\n",
        "        val_labels\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "fu2fwUX169pA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "CUe-pvlj7COI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 100\n",
        "\n",
        "model.fit(\n",
        "    {\n",
        "        'input_ids_written': np.array(train_features['input_ids_written']),\n",
        "        'attention_mask_written': np.array(train_features['attention_mask_written']),\n",
        "        'input_ids_stopwords': np.array(train_features['input_ids_stopwords']),\n",
        "        'attention_mask_stopwords': np.array(train_features['attention_mask_stopwords']),\n",
        "        'cluster_input': np.array(train_features['cluster']),\n",
        "    },\n",
        "    train_labels,\n",
        "    validation_data=(\n",
        "        {\n",
        "            'input_ids_written': np.array(val_features['input_ids_written']),\n",
        "            'attention_mask_written': np.array(val_features['attention_mask_written']),\n",
        "            'input_ids_stopwords': np.array(val_features['input_ids_stopwords']),\n",
        "            'attention_mask_stopwords': np.array(val_features['attention_mask_stopwords']),\n",
        "            'cluster_input': np.array(val_features['cluster']),\n",
        "        },\n",
        "        val_labels\n",
        "    ),\n",
        "    epochs=num_epochs,\n",
        "    callbacks=[f1_callback],\n",
        "    batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEpiIVau7EMx",
        "outputId": "2d4fef82-97cb-4031-97ae-d5df0ca49295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-07 23:14:06.916501: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:0').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "738/899 [=======================>......] - ETA: 27:14 - loss: 0.3220"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model"
      ],
      "metadata": {
        "id": "DKHQaunZy4VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model weights\n",
        "model.set_weights(f1_callback.best_weights)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model_save_name = 'deberta_emotion_classifier_tf'\n",
        "model.save_pretrained(model_save_name)\n",
        "tokenizer.save_pretrained(model_save_name)\n",
        "\n",
        "# Save emotion classes and best thresholds\n",
        "import json\n",
        "with open(os.path.join(model_save_name, 'emotion_classes.json'), 'w') as f:\n",
        "    json.dump(emotion_classes.tolist(), f)\n",
        "with open(os.path.join(model_save_name, 'optimal_thresholds.json'), 'w') as f:\n",
        "    json.dump(f1_callback.best_thresholds, f)\n",
        "\n",
        "print(f\"Model, tokenizer, and thresholds have been saved in '{model_save_name}'.\")"
      ],
      "metadata": {
        "id": "85S_VsQay2NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload to my huggingface"
      ],
      "metadata": {
        "id": "2uEEHxfvy5-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import HfApi\n",
        "\n",
        "# # Log in to Hugging Face\n",
        "# # If not already logged in earlier\n",
        "# # login(token='your_huggingface_token')\n",
        "\n",
        "# # Upload model\n",
        "# model.push_to_hub(model_save_name)\n",
        "# tokenizer.push_to_hub(model_save_name)\n",
        "\n",
        "# # Prepare dataset for upload\n",
        "# from datasets import Dataset\n",
        "\n",
        "# # Convert pandas DataFrame to Hugging Face Dataset\n",
        "# hf_dataset = Dataset.from_pandas(data)\n",
        "\n",
        "# # Save dataset locally\n",
        "# dataset_save_path = 'emotion_dataset_pt'\n",
        "# hf_dataset.save_to_disk(dataset_save_path)\n",
        "\n",
        "# # Upload dataset to Hugging Face\n",
        "# hf_dataset.push_to_hub('emotion_dataset_pt')\n",
        "\n",
        "# print(\"Model and dataset have been uploaded to Hugging Face Hub.\")"
      ],
      "metadata": {
        "id": "ECyLqD5Vy7sG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}